{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7eacfb1",
   "metadata": {},
   "source": [
    "# Reading the book "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d258c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"miracle_in_the_andes.txt\",\"r\") as file:\n",
    "    book = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a846fc49",
   "metadata": {},
   "source": [
    "# Processing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32524389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter class from collections has methods to perform counting on words in text \n",
    "from collections import Counter \n",
    "\n",
    "# Import regex module \n",
    "import re \n",
    "\n",
    "class Regex_processing:\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        \n",
    "    def find_word(self,regex_statement):\n",
    "        pattern = re.compile(regex_statement) \n",
    "        finding = re.findall(pattern,self.text) \n",
    "        return finding \n",
    "\n",
    "    def count_word(self,word):\n",
    "        # Compile words (add plus so you don't just get a list of letters)  \n",
    "        common_pattern = re.compile(\"[a-zA-Z]+\") \n",
    "\n",
    "        # findall \n",
    "        common_findings = re.findall(common_pattern,self.text.lower()) \n",
    "\n",
    "        # Loop through the common findings and count which is the most common word using Counter\n",
    "        word_count = [word for word in common_findings]\n",
    "        c = Counter(word_count) \n",
    "        return c[word] \n",
    "    \n",
    "    def most_common_word(self):\n",
    "        # Compile words (add plus so you don't just get a list of letters)  \n",
    "        common_pattern = re.compile(\"[a-zA-Z]+\") \n",
    "\n",
    "        # findall \n",
    "        common_findings = re.findall(common_pattern,self.text.lower()) \n",
    "\n",
    "        # Loop through the common findings and count which is the most common word using Counter\n",
    "        word_count = [word for word in common_findings]\n",
    "        c = Counter(word_count) \n",
    "        return c.most_common()\n",
    "    \n",
    "    def split_text(self, regex_statement):\n",
    "        # Compile the statement that is called to the method \n",
    "        pattern = re.compile(regex_statement) \n",
    "        \n",
    "        #Split the text \n",
    "        split_text = re.split(regex_statement,self.text) \n",
    "        \n",
    "        return split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "308899f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Object `\n",
    "rg = Regex_processing(book) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effd8130",
   "metadata": {},
   "source": [
    "# Find the most used words that aren't articles, i.e., 'a', 'the', etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b00ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42e75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of common article words in data \n",
    "english_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea7df43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('would', 575),\n",
       " ('us', 519),\n",
       " ('said', 292),\n",
       " ('roberto', 284),\n",
       " ('could', 252),\n",
       " ('one', 249),\n",
       " ('snow', 227),\n",
       " ('mountain', 183),\n",
       " ('time', 182),\n",
       " ('like', 165)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the articles from the word list \n",
    "\n",
    "filtered_words = []\n",
    "raw_word_lst = rg.most_common_word()\n",
    "\n",
    "# Loop over the raw list of the most common words, if the iteration is not in stopwords, add it to a new list \n",
    "for word,count in raw_word_lst:\n",
    "    if word not in english_stopwords:\n",
    "        filtered_words.append((word,count))\n",
    "        \n",
    "filtered_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eac1730",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: Find the most positive and negative chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee5a3939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sentiment intensity analyzer \n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "917a2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate intensity analyzer object \n",
    "analyzer = SentimentIntensityAnalyzer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9178a971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chapter 1',\n",
       " 'Chapter 2',\n",
       " 'Chapter 3',\n",
       " 'Chapter 4',\n",
       " 'Chapter 5',\n",
       " 'Chapter 6',\n",
       " 'Chapter 7',\n",
       " 'Chapter 8',\n",
       " 'Chapter 9',\n",
       " 'Chapter 10']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the chapters \n",
    "chapters = rg.split_text(\"Chapter [0-9]+\")\n",
    "\n",
    "# There is a whitespace at the start of the split chapters, remove this \n",
    "chapters = chapters[1:]\n",
    "\n",
    "# Get the title of each chapter \n",
    "chapter_title = rg.find_word(\"Chapter [0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7d4a055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1 = {'neg': 0.061, 'neu': 0.779, 'pos': 0.16, 'compound': 1.0}\n",
      "Chapter 2 = {'neg': 0.12, 'neu': 0.726, 'pos': 0.154, 'compound': 0.9991}\n",
      "Chapter 3 = {'neg': 0.145, 'neu': 0.751, 'pos': 0.105, 'compound': -0.9999}\n",
      "Chapter 4 = {'neg': 0.141, 'neu': 0.721, 'pos': 0.138, 'compound': -0.9963}\n",
      "Chapter 5 = {'neg': 0.118, 'neu': 0.742, 'pos': 0.141, 'compound': 0.9997}\n",
      "Chapter 6 = {'neg': 0.124, 'neu': 0.761, 'pos': 0.115, 'compound': -0.9979}\n",
      "Chapter 7 = {'neg': 0.136, 'neu': 0.761, 'pos': 0.103, 'compound': -0.9999}\n",
      "Chapter 8 = {'neg': 0.12, 'neu': 0.786, 'pos': 0.094, 'compound': -0.9998}\n",
      "Chapter 9 = {'neg': 0.097, 'neu': 0.824, 'pos': 0.079, 'compound': -0.9996}\n",
      "Chapter 10 = {'neg': 0.086, 'neu': 0.733, 'pos': 0.181, 'compound': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Loop through the chapters \n",
    "for individual_chapter,title in zip(chapters,chapter_title):\n",
    "    sentiment_score = analyzer.polarity_scores(individual_chapter)\n",
    "    print(f\"{title} = {sentiment_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c8b02",
   "metadata": {},
   "source": [
    "# Interpretation \n",
    "### We can see from the results that the book is written in a mostl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
